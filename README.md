# compute_layers
Repo meant to experiment and research how different kinds of AIs and algorithms lay down in the compute layers

# Introduction
The "bitter lesson" from Rich Sutton has invaded every corner of DaertML (and I guess every corner in the minds of anyone following the AI advancements during the last 13 years if we consider 2012 and the release of AlexNet as the origin of modern AI); somehow, certain recent tests in the lab have sparked the "what if we are doing everything wrong" opinion...

Before discussing the ideas in this repo, let's cover some terminology, and what really sparked the writing of this README.md:
- Im a big fan of the "abstraction layers" that are given if we consider writing any piece of code, using only 3 possible programming languages: assembly, C and Python; from left to right represent the lower level to higher level abstraction layers of programming. One can (relatively) easily inject assembly code inside C, and can convert a certain piece of code written in C as a working "module" in Python. In a similar manner, one can write C code, and call it from Python using the different mechanisms that we will not discuss here; thus, one can write higher level code, or lower level code in different sections, and is allowed to zoom in and zoom out the complexity of code by choosing a certain language for a certain use case. In this manner, optimizations can be left for later introduction, or can be leveraged at the beginning.

- This is a great abstraction that can be mapped to computing; in this manner, one can consider everything that is run in a computer from the OS, to the peripheral drivers, to the web browser of ChatGPT, a certain piece of code with certain data that behaves in a certain manner. Because of that, and thanks to the rise of what we will call from now on "AGIs" (making an abuse of notation): it is any AI model that is good enough to be general enough in different use cases. I guess, the first open attempts to achieve that, were around the release of Llama3.1; somehow, I like to consider GPT4, or evern GPT3.5, some kind of AGI (yep, sorry not the commercial kind of content that you are sold everyday); we can define the same kinds of abstraction layers for how compute happens.

It all started when trying to make AGIs play games like Super Mario Bros, or the classic gyms from OpenAI; it felt powerful to leave the agent in an environment with the aid of prompting and tools like code interpreter to allow it to build new tools on the go ("alla Voyager agent in Minecraft"); or with sensorial stimuli from VLMs, and reasoning capabilities from Thinking models... Somehow, the latency sucked, the compute cost skyrocketed, and the cost to achieve the same performance as other simpler mechanisms that are in a lower level in the compute stack, made the approach a useless experiment (somehow the idea is cool, and may get interesting results in the future).

The results of what local models can do in Atari environments are mixed: one can see the capabilities being strong in certain complex scenarios, but then realize that it all depends a lot in terms of the used model, and in many occasions a random set of inputs on the environment can lead you places... like always doing RIGHT on a platformer game is not a proof of intelligence, even though the LLM reaches a certain % of the level completion.

During these past months, I have felt DaertML (and the whole AI ecosystem) is going down the successful path taken by OpenAI since its inception; the need for RL environments is sparking again, the need to teach LLMs new capabilities and the opportunity to make our AGIs more general and more like an ASI is moving forward the ecosystem back to the classics. Somehow, now the common denominator used along the industry is the decoder only transformer and the language stack as a way to reason, think, plan and solve problems.

The bitter lesson may be failing us a bit, and the first results of it were found when trying to do multimodal training of models without using pretrained encoder and decoder and training a tiny layer in between them... Omni models are not a reality nowadays, and VLMs and Speech-to-text and text-to-speech is happening separately and being embedded into the language backbone. It is ok, it is alright, the bitter lesson doesnt break that early... we are still using backprop and search, but somehow, the most general method is not the chosen one to be scaled further. That is the part of the bitter lesson that most annoyed me during these last research months: "the most generic method, being a SOTA LLM wasnt being the best solution for the RL environments of Atari in the gymnasium".

