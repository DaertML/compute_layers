# compute_layers
Repo meant to experiment and research how different kinds of AIs and algorithms lay down in the compute layers

# Introduction
The "bitter lesson" from Rich Sutton has invaded every corner of DaertML (and I guess every corner in the minds of anyone following the AI advancements during the last 13 years if we consider 2012 and the release of AlexNet as the origin of modern AI); somehow, certain recent tests in the lab have sparked the "what if we are doing everything wrong" opinion...

Before discussing the ideas in this repo, let's cover some terminology, and what really sparked the writing of this README.md:
- Im a big fan of the "abstraction layers" that are given if we consider writing any piece of code, using only 3 possible programming languages: assembly, C and Python; from left to right represent the lower level to higher level abstraction layers of programming. One can (relatively) easily inject assembly code inside C, and can convert a certain piece of code written in C as a working "module" in Python. In a similar manner, one can write C code, and call it from Python using the different mechanisms that we will not discuss here; thus, one can write higher level code, or lower level code in different sections, and is allowed to zoom in and zoom out the complexity of code by choosing a certain language for a certain use case. In this manner, optimizations can be left for later introduction, or can be leveraged at the beginning.

- This is a great abstraction that can be mapped to computing; in this manner, one can consider everything that is run in a computer from the OS, to the peripheral drivers, to the web browser or ChatGPT, a certain piece of code with certain data that behaves in a certain manner. Because of that, and thanks to the rise of what we will call from now on "AGIs" (making an abuse of notation): it is any AI model that is good enough to be general enough in different use cases. I guess, the first open attempts to achieve that, were around the release of Llama3.1; somehow, I like to consider GPT4, or evern GPT3.5, some kind of AGI (yep, sorry not the commercial kind of content that you are sold everyday); we can define the same kinds of abstraction layers for how compute happens.

It all started when trying to make AGIs play games like Super Mario Bros, or the classic gyms from OpenAI; it felt powerful to leave the agent in an environment with the aid of prompting and tools like code interpreter to allow it to build new tools on the go ("alla Voyager agent in Minecraft"); or with sensorial stimuli from VLMs, and reasoning capabilities from Thinking models... Somehow, the latency sucked, the compute cost skyrocketed, and the cost to achieve the same performance as other simpler mechanisms that are in a lower level in the compute stack, made the approach a useless experiment (somehow the idea is cool, and may get interesting results in the future).

The results of what local models can do in Atari environments are mixed: one can see the capabilities being strong in certain complex scenarios, but then realize that it all depends a lot in terms of the used model, and in many occasions a random set of inputs on the environment can lead you places... like always doing RIGHT on a platformer game is not a proof of intelligence, even though the LLM reaches a certain % of the level completion.

During these past months, I have felt DaertML (and the whole AI ecosystem) is going down the successful path taken by OpenAI since its inception; the need for RL environments is sparking again, the need to teach LLMs new capabilities and the opportunity to make our AGIs more general and more like an ASI is moving forward the ecosystem back to the classics. Somehow, now the common denominator used along the industry is the decoder only transformer and the language stack as a way to reason, think, plan and solve problems.

The bitter lesson may be failing us a bit, and the first results of it were found when trying to do multimodal training of models without using pretrained encoder and decoder and training a tiny layer in between them... Omni models are not a reality nowadays, and VLMs and Speech-to-text and text-to-speech is happening separately and being embedded into the language backbone. It is ok, it is alright, the bitter lesson doesnt break that early... we are still using backprop and search, but somehow, the most general method is not the chosen one to be scaled further. That is the part of the bitter lesson that most annoyed me during these last research months: "the most generic method, being a SOTA LLM wasnt being the best solution for the RL environments of Atari in the gymnasium".

The other sign of need for specialized models appeared from the fact that using coding models for coding is better than using instruct models for coding, and the same happens for models that are finetuned on other fields. Even though one get a performance boost from mixing similar fields like coding and math, we should use our instinct to choose what things to put together in the training run, and which kinds of models were to be made.

# The compute layers
So, without further explaining the reasons that brought DaertML to this conclusion, here are the abstraction compute layers that we can consider:
algorithms -> search algorithms -> symbolic models -> subsymbolic models -> AGIs.

This classification is a bit off, as you will see in the following paragraphs, but you'll get the idea, here are the reasons to choose these abstraction compute layers:
- algorithm: a well defined (it doesnt need to be optimal) algorithm, specialized to solve a certain problem, will likely never be beaten by a trained by backprop model. In this category we can put any kind of algorithm,somehow developing algorithms need to be done only in such cases, in which he complexity and the speed of the method, as well as the needed calculations for it is something that beats by a large margin other methods up te abstraction layer classification. 
- search algorithms: this is the first symptom of this being a bad classification, but hold on. I like separating search algorithms like MuZero, MCTS, A star... from other algorithms that are more specific and naive. One can use MuZero and A star for way more things than if one programs a quick set of if-then-else statements that play a certain game in a quick and at high level performance. I would also consider genetic/evolutionary algorithms in this category.
- symbolic models: I refer to all such methods that resemble classic ML, and expert systems; or how you apply the different sets of rules that you consider to solve a certain problem at hand. I feel like this goes higher in the list of abstraction layers, as this requires a training mechanism, we start from an empty or unfinished set of rules and logic, and train it to be improved. On the other hand, the prior algorithms will perform in an equal manner at any situation in time.
- subsymbolic models: aka neural networks, this goes one step higher in the bitter lesson classification; in this case, we enter the master recipe to not get beaten by the bitter lesson, and being able to scale beyond the capabilities of the other methods. Any kind of neural network is welcomed in this category, thus it should not be so huge that doesnt allow compute efficiency and offers a good latency; no measure on what that means is given in this text, it should be considered for each use case at hand.
- AGIs: the highest abstraction layer, the one that you give it a prompt and a capture of the environment and it will make choices for you. Follows transformer/hybrid or huge LSTM based architectures. Its latency is high, requires tons of compute resources to run inference and to train; again, no thresholds on what each mean is given in this README.md.

So, knowing all the different players and their position in the complexity and generalism in the compute stack; let's discuss some examples that nowadays beat the idea that adding more generalism to out models is not going to bring us to better places in every possible use case:
- Latency critical environments: self driving has been solved already to a great performance; no one talks about safety needs, and we have taken for granted that there are self driving vehicles driving out there with better capabilities than humans. In this case, one should not run an LLM to get a response on what the wheel or pedals should do next: the compute needs, the generated heat, the latency, the lack of safety evaluations (what if the model stops, or just stays on thinking forever, or just refuses to answer...). The same category is shared with the already mentioned atari environments from the OpenAI gym.
- Problems solved with simpler mechanisms: plenty of problems have been solved with AI mechanisms prior to the release of AGIs, such methods in some cases required the generation of certain features by the engineer, or the training of models with prior preprocessing of the dataset, to adapt to the model architecture. If a simpler and lower level kind of model solves the problem, AGIs should not be used for this, as more cost will be introduced. Somehow, it is always welcomed to evaluate how AGIs behave in such cases, and how they could be used for synthetic data generation down the line.
- 
